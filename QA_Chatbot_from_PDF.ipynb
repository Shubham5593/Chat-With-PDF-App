{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEY-ZD7YZkwe"
      },
      "source": [
        "### Installing & Improting All required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VdgyTAASRN8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc475e85-be42-4016-9390-85395f55b98e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain google-generativeai langchain-google-genai python-dotenv unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P7WuHBAVYFSg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d08239ef-0983-42f4-8d61-d90119a30850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q unstructured_inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dRnxDhAUzR0",
        "outputId": "327c11e0-7f65-400e-a6f5-f000f798a312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\n",
            "pdfplumber 0.10.3 requires pdfminer.six==20221105, but you have pdfminer-six 20231228 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade -q pypdf pdf2image pdfminer.six pikepdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVu0EpOljeWZ",
        "outputId": "96ff33a3-050a-425e-b180-d238ca5f2d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_yO3BMsTSVcI"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "\n",
        "import pdf2image\n",
        "import pypdf\n",
        "from pdfminer import psparser\n",
        "from pdfminer.utils import open_filename\n",
        "import pikepdf\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import google.generativeai as genai\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory,ChatMessageHistory\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osOIhIR8Z1nY"
      },
      "source": [
        "### Initializing gemini pro LLM API KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QWUVGWYORSCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a52f4e0-9b15-4a55-d5cd-88048e2c1ee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIzaSyDXLOxtji7FGFO7S3k441AcKPjJKFxskrY··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# Defining Google API key for Acessing Gemini Pro LLM\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"AIzaSyDXLOxtji7FGFO7S3k441AcKPjJKFxskrY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef_16ZK5Z9LH"
      },
      "source": [
        "### Defining Prompt template & Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X0LpThqsVni4"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "    Answer the question  delimated in triple bracket as detailed as possible from the provided context given below delimated in triple bracket & using previous chat history of user,\n",
        "    make sure to provide all the details.\n",
        "    If the answer is not in provided context answer from your pretrained knowledge, don't provide the wrong answer.\n",
        "\n",
        "    Context:``` \\n {context} \\n ```\n",
        "\n",
        "    {chat_history}\n",
        "    Question: ``` \\n{question}\\n ```\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-pro\",\n",
        "                             temperature=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxWLFV2PaCoA"
      },
      "source": [
        "### Defining Memory to remembers past intereactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "UgJlm6I1WQPT"
      },
      "outputs": [],
      "source": [
        "history = ChatMessageHistory.construct()\n",
        "\n",
        "memory = ConversationBufferWindowMemory(llm=model,\n",
        "                                        memory_key=\"chat_history\",\n",
        "                                        input_key=\"question\",\n",
        "                                        chat_memory=history,\n",
        "                                        k=6,\n",
        "                                        return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhk4B75CaLYW"
      },
      "source": [
        "### Defining Chain - To wrap Prompt, Model & Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "6T3H-hdLWVv2"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"chat_history\", \"question\"])\n",
        "chain = load_qa_chain(model,  memory=memory, chain_type=\"stuff\", prompt=prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdz6gmD_aVc3"
      },
      "source": [
        "### Extracting PDF Data With help of PDF Document loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "UBYUAZS2RSEs"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
        "\n",
        "# Loading PDF data - Using Unstructured PDF Loader as PDF\n",
        "pdf_path = r\"/content/1.pdf\"\n",
        "loader = UnstructuredPDFLoader(pdf_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "UI_7uLHEWDf4"
      },
      "outputs": [],
      "source": [
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGM3D2dIab0G"
      },
      "source": [
        "### Spliting Entire PDF text Data into Chunks, As context window will not contain entire data so we are making chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "opV6X3ZqWDh-"
      },
      "outputs": [],
      "source": [
        "# Splitting Loaded PDF data into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=5000, chunk_overlap=700)\n",
        "chunks = text_splitter.split_documents(data)\n",
        "\n",
        "chunks = list(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf68HzkManCF"
      },
      "source": [
        "### Converting Chunks Text Data into Embeddings & storing in VectorDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "vedmhCpidaqK"
      },
      "outputs": [],
      "source": [
        " # Converting chunks text data into embeddings\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")\n",
        "vector_store = FAISS.from_documents(chunks, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYnJMzeZawp1"
      },
      "source": [
        "### Taking User Input & Invoking Chain for response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "c8o3cYaqm-ur"
      },
      "outputs": [],
      "source": [
        "user_question = \"How much is initial franchise fee?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "CjELoYp_lkuq"
      },
      "outputs": [],
      "source": [
        "docs = vector_store.similarity_search(user_question)\n",
        "\n",
        "response = chain(\n",
        "        {\"input_documents\":docs, \"question\": user_question}\n",
        "        ,return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "SXzQBINpoBhj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c08cb73-612e-4393-d7ef-04a7c5b73236"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output_text': '    The initial franchise fee is $20,000. However, there are reduced fees for veterans and existing owners. Active or honorably discharged military service members pay $10,000, franchise owners with one or more other Stores (but less than ten Stores) pay $10,000, and franchise owners with ten or more other Stores pay $5,000. These reduced fees are subject to change and do not apply to future franchises purchased by the franchise owner. All initial franchise fees are payable in full at the time of signing the agreement and are non-refundable.'}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjH5KRgga6ZQ"
      },
      "source": [
        "### Past interaction Memory For Remembering Conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "hii402WOYfEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7416f6-d28f-4417-c32b-0465b857b5bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatMessageHistory(messages=[HumanMessage(content='The total cost of investment required to begin BIGGBY cofee franchise?'), AIMessage(content='    The total estimated initial investment ranges from $202,450 to $418,700.'), HumanMessage(content='who is main Franchisor for BIGGBY Coffe?'), AIMessage(content='    The main Franchisor for BIGGBY Coffee is Global Orange Development, LLC.'), HumanMessage(content='How much is initial franchise fee?'), AIMessage(content='    The initial franchise fee is $20,000. However, there are reduced fees for veterans and existing owners. Active or honorably discharged military service members pay $10,000, franchise owners with one or more other Stores (but less than ten Stores) pay $10,000, and franchise owners with ten or more other Stores pay $5,000. These reduced fees are subject to change and do not apply to future franchises purchased by the franchise owner. All initial franchise fees are payable in full at the time of signing the agreement and are non-refundable.')])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "memory.chat_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "34Ni341aYq9F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}